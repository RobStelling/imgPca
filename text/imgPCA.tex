\documentclass{article} % For LaTeX2e
\usepackage{iclr2018_conference,times}
\usepackage{hyperref}
\usepackage{url}
\usepackage{natbib}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{subcaption}
\graphicspath{ {images/} }	

\title{ImgPCA \\ A data visualization tool for PCA Analysis \\ MAI718 3/2017 Final project}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Roberto Stelling \thanks{ PPGI/UFRJ Graduate Student, \href{http://stelling.cc}{http://stelling.cc}.} \\
PPGI - 117.335.792\\
Universidade Federal do Rio de Janeiro\\
Rio de Janeiro, RJ 21941-916, Brazil \\
\texttt{roberto@stelling.cc}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}
\maketitle
\pagestyle{plain}

\begin{abstract}
Traditionally PCA Analysis for dimension reduction in images is performed without a systematic visual aid approach. This paper describes a data visualization tool for PCA Analysis implemented in JavaScript aiming to improve the analyst decision process.
\end{abstract}

\section{Problem description}
There are many reasons to reduce a data set, some examples are: noise reduction, outlier removal, lossy image compression or even as a preliminary step in various types of data exploration and data analysis.\par
Principal component analysis (PCA) can be used as a lossy image compression solution, as you can apply PCA on a set of images of a data set and reduce its dimensionality with a certain, desirably controllable, loss of precision. Of course, one want to loose as little precision as possible while compressing as much as possible.With images, the main difficulty resides in the trade off between compression and precision: how many dimensions can be thrown away making sure that the resulting images still retain the desired level of quality or sharpness? Is there a general rule where you can certainly decide how many dimensions will be cut off the original data set? Of course, the type and amount of data available for compression, the data set, has a big influence on the precise point where the cut will happen, but can the analyst decide simply on the number of dimensions or amount of variance that will be thrown away and be sure that the results will be satisfactory? We propose that using a visual helping tool during the decision process can have a positive impact on the cut off selection.

\section{Brief introduction to PCA}
\subsection{Objective}
According to \citet{jolliffe1986principal}, the central idea of principal component analysis (PCA) is to reduce the dimensionality of a data set in which there are a large number of interrelated variables, while retaining as much as possible of the variation present in the data set.\par
This reduction is achieved by transforming the data set into a new set of variables, the principal components, which are not correlated, and which are ordered so that the first few retain most of the variation present in all the original variables.\par
\subsection{Brief History of PCA}
Principal component analysis was first described by \citet{pearson1901principal} and later developed independently by \citet{hotelling1933analysis} \citep{jolliffe1986principal}.\par
\citet{preisendorfer1988principal} state that in 1873, the Italian geometer Beltrami, formulated a modern form of the resolution of a general square matrix into its singular value decomposition (SVD), the decomposition that stands at the base of PCA.\par
\citet{craw1992face} describe a method for face recognition using principal component analisys, showing that PCA can be used as an effective tool in image analysis.
\subsection{Intuition}
PCA can be thought of as the problem of fitting an $n$-dimensional ellipsoid to the $m$-dimensional data, where $n<=m$ and each axis of the ellipsoid represents a principal component. The larger the axis of a component, the larger the variance for that component. So, the objective of PCA is to build a transformation of $m$-dimensional space to $n$-dimensional space while preserving most of the the $m$-dimensional space variance. To find that transformation and the components, we compute the singular value decomposition of the data. The singular value decomposition will provide a computationally efficient method of finding the principal components and the scaled versions of the principal component scores.
\subsection{Singular Value Decomposition}
Given an arbitrary $D_{m\times n}$ matrix, then $D$ can be written as
\begin{equation} \label{eq:SVD}
D_{m\times n}=U_{m\times r}S_{r\times r}V_{r\times n}^T
\end{equation}
where
\begin{enumerate}
\item[(i)] $U$ and $V$, each of which with orthonormal\footnote{both orthogonal and normalized} columns so that $U^TU=I_{r}$, $V^TV=I_{r}$;
\item[(ii)] $S$ is a diagonal matrix;
\item[(iii)] $r$ is the rank\footnote{corresponds to the maximal number of linearly independent columns of D} of $D$.
\end{enumerate}
$S$ is a diagonal matrix such as:
\[
  S =
  \begin{bmatrix}
    s_{1} & & \\
    & \ddots & \\
    & & s_{r}
  \end{bmatrix}
\]
Where $s_{1}$ to $s_{r}$ are the principal components scores and $s_{1} >= s_{2} >= ... >= s_{r-1} >= s_{r}$.\par
$U$ is the eigenvector matrix, with eigenvectors ordered by the component scores, their eigenvalues.
\subsection{Principal Component Analysis}
To apply PCA we will use equation (\ref{eq:SVD}) from the singular value decomposition and apply it to the covariance matrix $\Sigma$ of a data set. If the data set $D$ has dimensions ${m\times n}$ then the covariance matrix $\Sigma$ will be a symmetrical square matrix of dimensions $n\times n$. So
\begin{equation}\label{eq:SigmaLongSVD}
\Sigma_{n\times n} = U_{n\times n}S_{n\times n}V_{n\times n}^T
\end{equation}
Where
\begin{itemize}
\item $U$ is orthonormal and holds $\Sigma$ eigenvectors
\item $S$ is a diagonal matrix with $s_1 ... s_n$ as the descending ordered eigenvalues.
\end{itemize}
As $U$ is orthonormal then we can transform the original $D$ data set into $P$
\[ P_{m\times n} = D_{m\times n}U_{n\times n} \] and restore it back with:
\[P U^T = DUU^T = DI_{n} = D\]
$P$ is a transformation of $D$ that retains all the information of the original data set.

\section{PCA for image compression}
A computer image is usually thought of as a two dimensional matrix, with $m$ lines and $n$ columns representing the horizontal and vertical pixels of the image. A simpler, albeit equally meaningful, representation is a single vector with $m\times n$ cells for the whole image. The content of each cell, in either representation, depends on the selected image mode. For example: RGB, RGB grayscale, CMYK, etc. For the purposes of the following argument and the solution implementation, we assume that each cell is an integer between 0 and 255, representing the grayscale RGB value of the corresponding pixel. We will also assume that our data set has $m$ samples of $n$ pixels; if $h$ is the number of horizontal pixels and $v$ is the number of vertical pixels, then $n=h\times v$.\par
Lets assume that $D$ is a data set with $m$ images where each image has $n$ pixels. Computationally the method can work even if $m<n$ but it is recommended that $m >= n$, as that will result in better compression gains and finer eigenvector tuning.\par
Given that $D_{m\times n}$ is a data set with $m$ data points $\in \mathbb{R}^n$ where $m >= n$. Then we define ${D}_{m\times n}^*$ as the normalized data set, \[D^* = \frac{D-\overline{D}}{s}\] 
where $\overline{D}$ is the mean of $D$ and $s$ is the sample standard deviation of D. Let $\Sigma_{n\times n}$ be the covariance matrix of $D^{*}_{m\times n}$
\begin{equation}\label{eq:sigma}
 \Sigma = \frac{1}{m}D^{*T}D^*
\end{equation}

 Then, according to (\ref{eq:SVD}) and (\ref{eq:SigmaLongSVD}), the singular value decomposition of $\Sigma$ is: 
\begin{equation}\label{eq:sigmaSVD}
\Sigma = US V^T
\end{equation}
where:
\begin{itemize}
\item $U$ is an $n\times n$ orthonormal matrix
\item $S$ is an $n\times n$ diagonal matrix with non-negative numbers on the diagonal
\item $V$ is an $n\times n$ unitary matrix and $V^T$ is $V$ transposed.
\end{itemize}

\subsection{Reducing dimensions}
Given the original data set, $D_{m\times n}$ and $U_{n\times n}$ obtained from $\Sigma$ as per (\ref{eq:sigmaSVD}), then we can build a new reduced data set $P_{m\times k}$ with the first $k<n$ eigenvectors. This new data set is computed as:
\begin{equation}\label{eq:ReducingDimensions}
P_{m\times k} = D_{m\times n} U_{n\times k}
\end{equation}
where $U_{n\times k}$, or $U_k$, is the eigenvector matrix truncated to the first $k$ eigenvectors. The information contained on the truncated $(n-k)$ columns is lost in this transformation.
\subsection{Restoring data}
The transformation in (\ref{eq:ReducingDimensions}) is lossy, meaning that the information on the truncated $n-k$ columns is utterly lost during the transformation. Although it is possible to restore $P$ back to $D$ dimensions, the result will not be exactly $D$ but rather an approximation of $D$. The whole rationale of using PCA for image compression is that the last $n-k$ eigenvectors will hold as little variance as possible and the recovered images will be an acceptable approximation of the original images.
To transform $P$ back into $D$ space we compute \[P_{m\times k}U_{k\times n}^T = D_{m\times n}U_{n\times k}U_{k\times n}^T \approx D_{m\times n}\]
\section{How to select the number of components to retain}
The problem of selecting how many components to retain is not new, \citet{zwick1986comparison}, present the results of a Monte Carlo evaluation of five methods that have been proposed for determining how many factors or components to retain: Horn's parallel analysis, Velicer's minimum average partial, Cattell's scree test, Bartlett's chi-square test, and Kaiser's eigenvalue greater than $1.0$ rule. 
The determination of the number of components or factors to retain is likely to be the most important decision a researcher will make \citep{zwick1986comparison}.\par

We propose that a graphical supporting tool, with a graph similar to Cattell's scree plot but displaying $\log_{10} eigenvalues$ instead of $eigenvalues$, plus on the fly representations of a subset of the compressed images, and a subset of the eigenimages, can be instrumental in the decision of how many dimensions must be retained. We suggest that the use of eigenimages can increase the analyst understanding of the variation and trends of main eigenvectors of the data set.\par
The number of dimensions to retain will eventually be a decision based on the supporting graphs and the recovered images. We include a couple of numeric measures on the cut off point:
\begin{itemize}
\item Accumulated variation retained.
\item Number of components retained.
\item Component score on the cut off point, the cut off point eigenvalue.
\end{itemize}
If we were to use a rule similar to Kaiser's Rule\footnote{Eigenvalues greater than one}, based on our experience with the proposed tool with $32\times 32$ grayscale images, then we would suggest a $\frac{1}{10}$ Kaiser's Rule: eigenvalues greater than $0.1$. Trials with a few data sets suggest that a "\emph{One Tenth Kaiser's Rule}" with eigenvalues $> 0.1$ is a reasonable trade off between compression and sharpness.

%\begin{figure}[h]
%\begin{center}
%%\framebox[4.0in]{$\;$}
%\fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
%\end{center}
%\caption{Sample figure caption.}
%\end{figure}

\begin{figure}[h]
\begin{center}
\includegraphics[width=\textwidth]{imgPca.png}
\end{center}
\caption{ImgPCA screenshot with a typical \emph{$log_{10}$ eigenvalues $\times$ \#of components} curve.}
\end{figure}
Note that the Kaiser's Rule cut occurs exactly where the graph crosses $0$ ($\log_{10} eigenvalue = 0$) and that our "\emph{One Tenth Kaiser's Rule}" occurs when $\log_{10} eigenvalue = -1$.
\section{ImgPCA}

For the next few examples we are going to use a subset of CIFAR-10 (\citet{krizhevsky2009learning}, chapter 3) images modified to fit our purposes. CIFAR-10 is a collection of $32\times 32$ color images that are available in python, Matlab and binary versions. We converted CIFAR-10 images to grayscale (see \nameref{materials}) and rotated the images $90$ degrees to produce the input files for our usage.\par

\begin{figure}[h]
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.9\linewidth]{CIFAR10All1024RecoveredNormalized}
  \caption{$302$ components}
  \label{fig:cifarall302}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.9\linewidth]{CIFAR10All1024RecoveredNormalized84}
  \caption{$84$ components}
  \label{fig:cifarall84}
\end{subfigure}
\caption{CIFAR-10 images recovered using "\emph{One Tenth}" (\ref{fig:cifarall302}) and regular Kaiser's rule (\ref{fig:cifarall84}).}
\label{fig:sampleCifar10}
\end{figure}

Figure \ref{fig:sampleCifar10} shows recovered images from a data set of $1024$ images with $1024$ ($32\times 32$) pixels of all CIFAR-10 classes, in the order they appear on the original file. Figure \ref{fig:cifarall302} shows the images recovered with $302$ components and figure \ref{fig:cifarall84} shows the images recovered $84$ components. The $302$ components  represent $98.786\%$ of variance, with the smallest component score being $0.1011$, following our "\emph{One Tenth Kaiser's Rule}". There are a few artifacts on the recovered images but the results seems to be visually satisfactory. The 84 components example on Figure \ref{fig:cifarall84} follows Kaiser's Rule of keeping eigenvectors with $eigenvalues > 1.0$. It is clear that cutting at this point, for our particular application, produces unsatisfactory approximations when dimensions are restored.\par

\begin{figure}[h]
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.9\linewidth]{CIFAR10All1024Eigenimages}
  \caption{From $1024$ images}
  \label{fig:eigenImagesAll1024}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.9\linewidth]{CIFAR10All15000Eigenimages}
  \caption{From $15000$ images}
  \label{fig:eigenImagesAll15000}
\end{subfigure}
\caption{Eigenimages generated from $1024$ and $15000$ images of all CIFAR-10 classes.}
\label{fig:eigenImagesAll}
\end{figure}

Table \ref{tab:ImgPCAcomputations} shows the stats when we apply the \emph{One tenth Kaiser's rule} to two datasets of different sizes. It is important to notice that all images of the first data set are included on the second. If we increase the number of images on the data set, from $1024$ to $15000$ images then we notice a few changes on ImgPCA statistics display, the most notable visual difference is the definition and ordering of the eigenimages.

\begin{table}[h]
\caption{Measures for CIFAR-10, all classes, at \emph{One tenth Kaiser's rule}}
\label{tab:ImgPCAcomputations}
\begin{center}
\begin{tabular}{lrr}
\multicolumn{1}{l}{\bf MEASURE}  &\multicolumn{1}{r}{\bf 1024 IMAGES}&\multicolumn{1}{r}{\bf 15000 IMAGES}
\\ \hline \\
Features included       &$302$ & $341$\\
Accumulated variance             &$98.787$\% & $98.329$\%\\
Last eigenvalue included&$0.1011$&$0.1007$\\
\end{tabular}
\end{center}
\end{table}

\subsection{Eigenimages}
Figure \ref{fig:eigenImagesAll} shows the first $72$ eigenimages generated from data sets of size $1024$ (Figure \ref{fig:eigenImagesAll1024}) and $15000$ (Figure \ref{fig:eigenImagesAll15000}) images of all CIFAR-10 classes. The eigenimages reveal some characteristics of the images on the data set: centrality of subject, dark central subject on a clear image, clear central subject on a dark image, left to right symmetry, bottom to top symmetry among others. It is not surprising that the ordering of the eigenimages changes when we increase or decrease the number of images of our test sample but the improvement of eigenimage definition, contrast and sharpness when the size of the data set increases is striking.

\begin{figure}[h]
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.9\linewidth]{CIFAR10C05000Eigenimages}
  \caption{Class 0 - Planes}
  \label{fig:eigenImagesC0}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.9\linewidth]{CIFAR10C35000Eigenimages}
  \caption{Class 3 - Cats}
  \label{fig:eigenImagesC3}
\end{subfigure}
\caption{Eigenimages generated from $5000$ images of two different classes.}
\label{fig:eigenImagesC0C3}
\end{figure}

The CIFAR-10 subset used to produce the displays in Figure \ref{fig:eigenImagesAll} includes images from all $10$ categories (numbered from $0$ to $9$) and has as much variance as possible given the chosen subject of the data set.\par In Figure \ref{fig:eigenImagesC0C3} we can see the eigenimages generated from 5000 images of CIFAR-10 class 0 (\ref{fig:eigenImagesC0}) and CIFAR-10 class 3 (\ref{fig:eigenImagesC3}). Not only the eigenimages are different but one can almost see vestigial planes on \ref{fig:eigenImagesC0} and vestigial cats on \ref{fig:eigenImagesC3}, the subject of their corresponding CIFAR-10 classes.
This effect, applied to faces, or eigenfaces, was observed and used by \citet{sirovich1987low}.

\subsection{ImgPCA plot}

\begin{figure}[h]
\begin{center}
\includegraphics[width=\textwidth]{imgPcaKaiserRule.png}
\end{center}
\caption{ImgPCA plot with Kaiser's Rule and \emph{One tenth Kaiser's rule} indicated.}
\label{fig:ImgPCAKaiserAndOTK}
\end{figure}

Instead of plotting an \textit{eigenvalues $\times$  \# of components} graph, as in the regular Cattell's scree plot, we propose plotting \textit{$\log_{10}$ (eigenvalues) $\times$ \# of components}. Similar to the scree plot, we can derive some understanding about the data based on how fast the $\log_{10}$ curve decays but we also have a quick visual and mathematical way of finding the Kaiser's Rule, that occurs when $\log_{10}$ \emph{(eigenvalues)} $= 0$. Additionally, every one tenth of eigenvalue decrease can also be easily found, as they correspond to the points $0, -1, -2$ etc. For example, to apply our \emph{One tenth Kaiser's rule} we only need to check what are the number of components that correspond to $\log_{10}$ \emph{(eigenvalues)} $= -1$, as shown in Figure \ref{fig:ImgPCAKaiserAndOTK}.

\begin{figure}[h]
\begin{center}
\includegraphics[width=\textwidth]{CIFAR10All512Plot.png}
\end{center}
\caption{ImgPCA plot of a data set with $512$ images of $1024$ pixels.}
\label{fig:ImgPCA512}
\end{figure}

Another feature of the \emph{$log_{10}$ eigenvalues $\times$ \#of components} curve is that it is very easy to spot the point where data is missing from a data set. Figure \ref{fig:ImgPCA512} shows a plot of data set of $512$ CIFAR-10 images. We see a big fall at $512$ components, where the remaining components are not as significant as the first half. Compare Figure \ref{fig:ImgPCA512} with the typical curve of Figure \ref{fig:ImgPCAKaiserAndOTK}.

\section{Materials and Methods}\label{materials}
\subsection{CIFAR-10 conversion}
All the data used on the text examples is available at \citet{Stelling:aa}, at the \texttt{/localData} folder. Images were taken from CIFAR-10 \citep{krizhevsky2009learning} but were converted to grayscale at rotated  for our usage. The conversion can be found at the \texttt{cifar2Gray.m} Octave routine in the \texttt{/dataGen} folder of the \citet{Stelling:aa} repository.\par

The core of the changes on CIFAR-10 images is the conversion to grayscale with luminance signal coefficients following ITU-R Recommendation BT.709 \citep{ITU-R:2015aa}, as shown in the following Octave code excerpt:
\begin{verbatim}
    gs_data=classData(1:nLines,1:1024)*0.2126 ...
          + classData(1:nLines,1025:2048)*0.7152 ...
          + classData(1:nLines,2049:3072)*0.0722;
\end{verbatim}
and the resulting image is finally rotated 90 degrees clockwise, as per the following Octave code.
\begin{verbatim}
        ndata(k,:) = ...
            reshape(rot90( ...
                reshape(gs_data(k,:), 32, 32), -1), 1, []);
\end{verbatim}
The resulting data sets are used as input on ImgPCA web application.
\subsection{SVD decomposition}
We used Numeric \citep{Loisel:2012aa} Javascript library to compute the SVD decomposition. Numeric is a stable numerical analysis library for Javascript. Computation results where in agreement with parallel computations performed in GNU Octave \citep{Eaton:2018aa}. One area for improvement in ImgPCA would be the usage of a GPU-enabled library for the SVD decomposition, instead of Numeric.
\subsection{Covariance matrix}
To accelerate the computation of the covariance matrix we used the GPU accelerated javascript library gpu.js \citep{Sapuan:2017aa}. To use gpu.js you need to create a kernel that will execute your code, as shown in the following JavaScript excerpt. The parameter for coVM a is the normalized data set matrix ($m\times n$). This kernel computes the covariance matrix, as defined by equation \ref{eq:sigma}.
\begin{verbatim}
const coVM = getData.gpu.createKernel(function(a) {
  var sum = 0;
  for (var i = 0; i < this.constants.size; i++) {
    sum += a[i][this.thread.x] * a[i][this.thread.y];
  }
  return sum/this.constants.size;
}
\end{verbatim}
\subsubsection*{Acknowledgments}
I'd wish to thank the whole of the MAI718/2017-3 class for their invaluable input during preliminary presentations of the current work. Special thanks go to our professor, Adriana Vivacqua, her ideas, the discussions she promoted and critiques to the preliminary versions where instrumental in improving the foundations of this work. 

I'd specially like to thank PPGI/UFRJ for letting me use their computer laboratory for the duration of the 2017 term. Without that access I wouldn't have the means to work on this project with the intensity it required.
\pagebreak
\bibliography{imgPCA}
\bibliographystyle{iclr2018_conference}

\end{document}
