\documentclass{article} % For LaTeX2e
\usepackage{iclr2018_conference,times}
\usepackage{hyperref}
\usepackage{url}
\usepackage{natbib}
\usepackage{amsfonts}
\usepackage{amsmath}

\title{A data visualization for PCA Analysis / \\ DataViz Final project}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Roberto Stelling \thanks{ PPGI/UFRJ Graduate Student, \href{http://stelling.cc}{http://stelling.cc}.} \\
PPGI\\
Universidade Federal do Rio de Janeiro\\
Rio de Janeiro, RJ 21941-916, Brazil \\
\texttt{roberto@stelling.cc}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}
\maketitle

\begin{abstract}
Traditionally PCA Analysis for features reduction in images is performed without a systematic visual aid approach. This paper describes the implementation of a PCA Analysis tool in JavaScript that uses data visualization techniques aiming to improve the analysis process.
\end{abstract}

\section{Problem description}
There are many reasons to reduce a data set, some modern examples are: noise reduction, outlier removal, lossy image compression or even as a preliminary step in various types of data exploration and data analysis.\par
Principal component analysis (PCA) is well suited as a lossy image compression solution, as you can apply PCA on a set of points of a data set and reduce its dimensionality with a certain, desirably controllable, loss of precision. Of course, one want to loose as little precision as possible while compressing as much as possible. There is a clear trade off between compression and precision. With images, the main difficulty resides in this very trade off: how many dimensions can the algorithm throw away and still retain the desired level of image quality or sharpness? Is there a general rule where you can certainly decide how many dimensions will be cut off the original data set? Of course, the type and amount of data available for compression, the data set, has a big influence on the final point where the cut will be, but can the analyst decide simply on the number of dimensions or amount of variance that will be thrown away and be sure that the results will be satisfactory? We propose that using a visual helping tool during the decision process can have a positive impact on the cut off selection.

\section{Brief introduction to PCA}
\subsection{Objective}
According to \citet{jolliffe1986principal}, the central idea of principal component analysis (PCA) is to reduce the dimensionality of a data set in which there are a large number of interrelated variables, while retaining as much as possible of the variation present in the data set.\par
This reduction is achieved by transforming the data set to a new set of variables, the principal components, which are not correlated, and which are ordered so that the first few retain most of the variation present in all the original variables.\par Principal component analysis was first described by \citet{pearson1901principal} and later developed independently by \citet{hotelling1933analysis} \citep{jolliffe1986principal}.\par
\citet{craw1992face} describe a method for face recognition using principal component analisys, showing that PCA can be used as an effective tool in image analysis.
\subsection{Intuition}
PCA can be thought of as the problem of fitting an $n$-dimensional ellipsoid to the $m$-dimensional data, where $n<=m$ and each axis of the ellipsoid represents a principal component. The larger the axis of a component, the larger the variance for that component. So, the objective of PCA is to build a transformation of $m$-dimensional space to $n$-dimensional space while preserving most of the the $m$-dimensional space variance. To find that transformation and the components, we compute the singular value decomposition of the data. The singular value decomposition will provide a computationally efficient method of finding the principal components and the scaled versions of the principal component scores.
\subsection{Singular Value Decomposition}
Given an arbitrary $D_{m\times n}$ matrix, then $D$ can be written as 
\begin{equation} \label{eq1}
D=USV^T
\end{equation}
where
\begin{enumerate}
\item[(i)] $U_{m\times r}$, $V_{n\times r}$ each of which with orthonormal\footnote{both orthogonal and normalized} columns so that $U^TU=I_{r}$, $V^TV=I_{r}$;
\item[(ii)] $S_{r\times r}$ is a diagonal matrix;
\item[(iii)] $r$ is the rank\footnote{corresponds to the maximal number of linearly independent columns of D} of $D$.
\end{enumerate}
$S$ is a diagonal matrix such as:
\[
  S =
  \begin{bmatrix}
    s_{1} & & \\
    & \ddots & \\
    & & s_{r}
  \end{bmatrix}
\]
$s_{1}$ to $s_{r}$ are the principal components scores and $s_{1} >= s_{2} >= ... >= s_{r-1} >= s_{r}$.\par
$U$ is the eigenvector matrix, with eigenvectors ordered by the component scores.
\section{PCA for image compression}
Let $D_{m\times n}$ be a data set with $m$ data points $\in \mathbb{R}^n$ where $m >= n$. We define $\widehat{D}_{m\times n}$ as the normalized data set, \[\widehat{D} = \frac{D-\overline{D}}{s}\] Let $\Sigma_{n\times n} = cov(\widehat{D})$ be the covariance matrix of $\widehat{D}$ \[\Sigma = \frac{1}{m}\widehat{D}^T\widehat{D}\] Then the singular value decomposition of $\Sigma$ is, according to equation (\ref{eq1}): \[\Sigma = US V^T\] where:
\begin{itemize}
\item $U$ is an $n\times n$ unitary matrix\footnote{If $U$ is unitary then $U^*U=UU^*=I$}
\item $S$ is an $n\times n$ diagonal matrix with non-negative numbers on the diagonal
\item $V$ is an $n\times n$ unitary matrix and $V^T$ is $V$ transposed.
\end{itemize}

\section{How to select the number of components to retain}
The problem of selecting how many components to retain is not new, \citet{zwick1986comparison}, present the results of a Monte Carlo evaluation of five methods that have been proposed for determining how many factors or components to retain: Horn's parallel analysis, Velicer's minimum average partial [MAP], Cattell's scree test, Bartlett's chi-square test, and Kaiser's eigenvalue greater than 1.0 rule. 
The determination of the number of components or factors to retain is likely to be the most important decision a researcher will make \citep{zwick1986comparison}.


\section{Citations, figures, tables, references}
\label{others}

These instructions apply to everyone, regardless of the formatter being used.

\subsection{Citations within the text}


Citations within the text should be based on the \texttt{natbib} package
and include the authors' last names and year (with the ``et~al.'' construct
for more than two authors). When the authors or the publication are
included in the sentence, the citation should not be in parenthesis (as
in ``See \citet{Goodfellow:2016aa} for more information.''). Otherwise, the citation
should be in parenthesis (as in ``Deep learning shows promise to make progress towards AI~\citep{Goodfellow:2016aa}.'').

The corresponding references are to be listed in alphabetical order of
authors, in the \textsc{References} section. As to the format of the
references themselves, any style is acceptable as long as it is used
consistently.

\subsection{Footnotes}

Indicate footnotes with a number\footnote{Sample of the first footnote} in the
text. Place the footnotes at the bottom of the page on which they appear.
Precede the footnote with a horizontal rule of 2~inches
(12~picas).\footnote{Sample of the second footnote}

\subsection{Figures}

All artwork must be neat, clean, and legible. Lines should be dark
enough for purposes of reproduction; art work should not be
hand-drawn. The figure number and caption always appear after the
figure. Place one line space before the figure caption, and one line
space after the figure. The figure caption is lower case (except for
first word and proper nouns); figures are numbered consecutively.

Make sure the figure caption does not get separated from the figure.
Leave sufficient space to avoid splitting the figure and figure caption.

You may use color figures.
However, it is best for the
figure captions and the paper body to make sense if the paper is printed
either in black/white or in color.
\begin{figure}[h]
\begin{center}
%\framebox[4.0in]{$\;$}
\fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
\end{center}
\caption{Sample figure caption.}
\end{figure}

\subsection{Tables}

All tables must be centered, neat, clean and legible. Do not use hand-drawn
tables. The table number and title always appear before the table. See
Table~\ref{sample-table}.

Place one line space before the table title, one line space after the table
title, and one line space after the table. The table title must be lower case
(except for first word and proper nouns); tables are numbered consecutively.

\begin{table}[t]
\caption{Sample table title}
\label{sample-table}
\begin{center}
\begin{tabular}{ll}
\multicolumn{1}{c}{\bf PART}  &\multicolumn{1}{c}{\bf DESCRIPTION}
\\ \hline \\
Dendrite         &Input terminal \\
Axon             &Output terminal \\
Soma             &Cell body (contains cell nucleus) \\
\end{tabular}
\end{center}
\end{table}




\subsubsection*{Acknowledgments}

Use unnumbered third level headings for the acknowledgments. All
acknowledgments, including those to funding agencies, go at the end of the paper.

\bibliography{imgPCA}
\bibliographystyle{iclr2018_conference}

\end{document}
